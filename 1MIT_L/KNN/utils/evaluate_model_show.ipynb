{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "evaluate_model_show.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkIURPrSABP-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import operator\n",
        "import os\n",
        "from psutil import cpu_count\n",
        "import string\n",
        "import time\n",
        "\n",
        "from collections import Counter,defaultdict\n",
        "from keras.models import model_from_json \n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd20l9FFsv-i",
        "colab_type": "text"
      },
      "source": [
        "Download selected test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oM9o_cOQs3vs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Choose dataset to evaluate\n",
        "reid = True\n",
        "hdr = False\n",
        "\n",
        "# TODO: Path to model and weights\n",
        "path_to_model   = '/content/drive/My Drive/KNN/model-ctc.json'\n",
        "path_to_weights = '/content/drive/My Drive/KNN/weights-ctc.hdf5'\n",
        "\n",
        "# TODO: Path to datasets\n",
        "if hdr:\n",
        "    zip_path = '/content/drive/My Drive/KNN/HDR_dataset.zip'\n",
        "    !cp \"{zip_path}\" .\n",
        "    !unzip -q -n 'HDR_dataset.zip' -d './HDR'\n",
        "if reid:\n",
        "    zip_path = '/content/drive/My Drive/KNN/2017-IWT4S-CarsReId_LP-dataset.zip'\n",
        "    !cp \"{zip_path}\" .\n",
        "    !unzip -q -n '2017-IWT4S-CarsReId_LP-dataset.zip' -d './REID'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlDzeAbhoufS",
        "colab_type": "text"
      },
      "source": [
        "read test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhhup9who8T6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_blank_chars(label):\n",
        "    empty_cnt = 8 - len(label)\n",
        "    empty_c = ''\n",
        "    for i in range(0, empty_cnt):\n",
        "        empty_c += '#'\n",
        "    second_part_beg = len(label) - 4\n",
        "    new_end = empty_c + label[second_part_beg:]\n",
        "    first_part_delim = 8 - len(new_end)\n",
        "    return label[:first_part_delim] + new_end\n",
        "\n",
        "\n",
        "\n",
        "if hdr:\n",
        "    hdr_dataset_images = []\n",
        "    hdr_dataset_labels = []\n",
        "    with open('HDR/trainVal.csv', 'r') as csv_file:\n",
        "        csv_reader = csv.reader(csv_file, delimiter=';')\n",
        "        line_count = 0\n",
        "        for row in csv_reader:\n",
        "            if line_count == 0:\n",
        "                # Skip header row\n",
        "                line_count += 1\n",
        "            else:\n",
        "                if (len(row[2]) != 8):\n",
        "                    label = add_blank_chars(row[2])\n",
        "                else:\n",
        "                    label = row[2]\n",
        "                img = cv2.imread(os.path.join('./HDR',row[1][2:]))\n",
        "                if img is None:\n",
        "                    continue\n",
        "                img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "                normalizedImg = np.zeros((200, 40))\n",
        "                normalizedImg = cv2.normalize(img,  normalizedImg, 0, 255, cv2.NORM_MINMAX)\n",
        "                res = cv2.resize(normalizedImg, dsize=(200, 40), interpolation=cv2.INTER_CUBIC)\n",
        "                hdr_dataset_labels.append([label])\n",
        "                hdr_dataset_images.append(res)\n",
        "\n",
        "    hdr_dataset_images = hdr_dataset_images[:50]\n",
        "    hdr_dataset_labels = hdr_dataset_labels[:50]\n",
        "    hdr_dataset_images = np.asarray(hdr_dataset_images)\n",
        "    hdr_dataset_labels = np.asarray(hdr_dataset_labels)\n",
        "\n",
        "if reid:\n",
        "    reid_dataset_images = []\n",
        "    reid_dataset_labels = []\n",
        "    with open('REID/trainVal.csv', 'r') as csv_file:\n",
        "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "        line_count = 0\n",
        "        for row in csv_reader:\n",
        "            if line_count == 0:\n",
        "                # Skip header row\n",
        "                line_count += 1\n",
        "            else:\n",
        "                if int(row[3]) == 0:\n",
        "                    if (len(row[2]) != 8):\n",
        "                        label = add_blank_chars(row[2])\n",
        "                    else:\n",
        "                        label = row[2]\n",
        "                    img = cv2.imread(os.path.join('./REID',row[1]))\n",
        "                    if img is None:\n",
        "                        continue\n",
        "                    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "                    normalizedImg = np.zeros((200, 40))\n",
        "                    normalizedImg = cv2.normalize(img,  normalizedImg, 0, 255, cv2.NORM_MINMAX)\n",
        "                    res = cv2.resize(normalizedImg, dsize=(200, 40), interpolation=cv2.INTER_CUBIC)\n",
        "                    reid_dataset_labels.append([label])\n",
        "                    reid_dataset_images.append(res)\n",
        "\n",
        "    reid_dataset_images = reid_dataset_images[:50]\n",
        "    reid_dataset_labels = reid_dataset_labels[:50]\n",
        "    reid_dataset_images = np.asarray(reid_dataset_images)\n",
        "    reid_dataset_labels = np.asarray(reid_dataset_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eiGYWUPIAbp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_some_examples(images, labels):\n",
        "    plt.figure(figsize=(10,10))\n",
        "    for i in range(10):\n",
        "        plt.subplot(5,5,i+1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.grid(False)\n",
        "        plt.imshow(images[i], cmap=plt.cm.binary)\n",
        "        plt.xlabel(labels[i])\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "if hdr:\n",
        "    print('=================hdr=============================')\n",
        "    show_some_examples(hdr_dataset_images, hdr_dataset_labels)\n",
        "\n",
        "if reid:\n",
        "    print('=================reid=============================')\n",
        "    show_some_examples(reid_dataset_images, reid_dataset_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rr6AMNjf1WQM",
        "colab_type": "text"
      },
      "source": [
        "Evaluate accuracy of model on test dataset, calculate and print statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vha5iCR6oope",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# decode to license plate characters\n",
        "map_numbers_to_char = {}\n",
        "for i,d in enumerate(string.digits+string.ascii_uppercase + '#'):\n",
        "    map_numbers_to_char[i] = d\n",
        "\n",
        "# transform nn output into list with license plate characters\n",
        "def get_readable_prediction(nn_output):\n",
        "    idxs = [np.argmax(x) for x in nn_output]\n",
        "    lp = [map_numbers_to_char[idx] for idx in idxs]\n",
        "    return lp\n",
        "\n",
        "\n",
        "# comapre two lists, count statistics \n",
        "def is_prediction_right(lstPrediction, lstLabel, statistics):\n",
        "    lstPrediction = np.array(lstPrediction)\n",
        "    lstLabel = np.array(lstLabel)\n",
        "    # debug\n",
        "    # print('lstPrediction:', lstPrediction)\n",
        "    # print('lstLabel     :', lstLabel)\n",
        "    if (lstPrediction==lstLabel).all():\n",
        "        statistics['total_chars'] += len(lstPrediction)\n",
        "        return True\n",
        "    else:\n",
        "        for j, char in enumerate(lstLabel):\n",
        "            if char != lstPrediction[j]:\n",
        "                statistics['branch_list'][j].append(char)\n",
        "                statistics['chars_count'] += 1\n",
        "            statistics['total_chars'] += 1\n",
        "        return False\n",
        "\n",
        "\n",
        "def test_weights(loaded_model, images, labels, chars_stats_counter, get_chars_stats=False):\n",
        "    total = len(images)\n",
        "    same_as_label = 0\n",
        "    branch_list = [[] for _ in range(8)]\n",
        "    statistics = {'chars_count': 0, 'total_chars': 0, 'branch_list':branch_list}\n",
        "\n",
        "    # # debug\n",
        "    # print (statistics)\n",
        "    # print(list(labels[0][0]))\n",
        "    # print(get_readable_prediction(prediction_1_lp))\n",
        "\n",
        "    time_measurements = {'start':0,'end':0}\n",
        "    for i, image in enumerate(images):\n",
        "        start = time.clock()  \n",
        "        lp_prediction = loaded_model.predict(np.asarray([image]))\n",
        "        end = time.clock()\n",
        "        time_measurements['start'] += start\n",
        "        time_measurements['end'] += end\n",
        "        readable_prediction = get_readable_prediction(lp_prediction)\n",
        "\n",
        "        if is_prediction_right(\n",
        "            readable_prediction, \n",
        "            list(labels[i][0]),\n",
        "            statistics\n",
        "            ): \n",
        "            same_as_label += 1\n",
        "\n",
        "        from google.colab.patches import cv2_imshow\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "        cv2_imshow(image)\n",
        "        print('Prediction: {}'.format(str(readable_prediction)))\n",
        "        print('Ground truth: {}'.format(str(labels[i][0])))\n",
        "        print('------------------------------------------')\n",
        "        print('')\n",
        "\n",
        "    print(\"successfuly recognized: \", same_as_label)\n",
        "    print(\"total: \", total)\n",
        "    success_rate = same_as_label/total*100\n",
        "    print(\"successful rate: {}%\".format(str(round(success_rate, 2))))\n",
        "\n",
        "    print(\"error rate [%] (character/license plate): {}/{}\".format(\n",
        "        str(round(\n",
        "            statistics['chars_count']/statistics['total_chars']*100,\n",
        "            2\n",
        "            )),\n",
        "         str(round((100 - success_rate), 2)),\n",
        "    ))\n",
        "    time_per_image = ((time_measurements['end'] - time_measurements['start'])\n",
        "                      /len(images))\n",
        "    print(\"Time per image: {} \".format( str(round(time_per_image, 10)))) \n",
        "\n",
        "    chars_stats = [Counter(x )for x in statistics['branch_list']]\n",
        "    if get_chars_stats:\n",
        "        chars_stats_counter.append(chars_stats)\n",
        "    return success_rate\n",
        "\n",
        "\n",
        "# print(statistics)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKuowxo77btn",
        "colab_type": "text"
      },
      "source": [
        "Load every model and weights and print all of them and the best one at the end"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAIEHdV27dZ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "range_from = 1 # Nemenit\n",
        "range_to = 2   # Nemenit\n",
        "\n",
        "def run_statistics(images, labels, dataset_name, range_from, range_to):\n",
        "    chars_stats_counter = []\n",
        "    best_weights = []\n",
        "    print('====================================================')\n",
        "    for i in range(range_from,range_to):\n",
        "        start = time.clock()  \n",
        "        # load json and create model\n",
        "        json_file = open(path_to_model, 'r')\n",
        "        loaded_model_json = json_file.read()\n",
        "        json_file.close()\n",
        "        loaded_model = model_from_json(loaded_model_json)\n",
        "        # load weights into new model\n",
        "        loaded_model.load_weights(path_to_weights)\n",
        "        print('-------------dataset {}, model {}------------------'.format(dataset_name, i))\n",
        "        best_weights.append(test_weights(loaded_model, images, labels, chars_stats_counter))\n",
        "        end = time.clock()\n",
        "        print('this calculation takes {}'.format(end-start))\n",
        "    idx = best_weights.index(max(best_weights))\n",
        "    idx += range_from  # numbering from 1 not from zero\n",
        "    print()\n",
        "    print('****************************************************')\n",
        "    print('best model index:', idx)\n",
        "    json_file = open(path_to_model, 'r')\n",
        "    loaded_model_json = json_file.read()\n",
        "    json_file.close()\n",
        "    loaded_model = model_from_json(loaded_model_json)\n",
        "    # load weights into new model\n",
        "    loaded_model.load_weights(path_to_weights)\n",
        "    print('-------------dataset {}, model {}------------------'.format(dataset_name, idx))\n",
        "    _ = test_weights(loaded_model, images, labels, chars_stats_counter, True)\n",
        "    print('****************************************************')\n",
        "    print()\n",
        "    return chars_stats_counter\n",
        "\n",
        "if hdr:\n",
        "    chars_stats_counter_hdr = run_statistics(hdr_dataset_images, hdr_dataset_labels, 'hdr', range_from, range_to)\n",
        "if reid:\n",
        "    chars_stats_counter_reid = run_statistics(reid_dataset_images, reid_dataset_labels, 'reid', range_from, range_to)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFYQcTus2OLs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get hardware info\n",
        "print(cpu_count())\n",
        "!cat /proc/cpuinfo | grep 'model name'\n",
        "!nvidia-smi -L\n",
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uStP_w4zHxbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_all_used_chararcters(labels):\n",
        "    all_used_chars = []\n",
        "    for item in labels:\n",
        "        chars = list(item[0])\n",
        "        for char in chars:\n",
        "            if char not in all_used_chars:\n",
        "                all_used_chars.append(char)\n",
        "    return all_used_chars\n",
        "\n",
        "def character_analysis(c, all_labeled_chars=None):\n",
        "    # c = [Counter({'8': 9, '9': 6, '2': 6, '7': 6, '1': 5, 'B': 4, '5': 4, '6': 4, '3': 3, '4': 2, 'L': 2, 'F': 2, 'D': 2, 'S': 2, 'Z': 1}), Counter({'B': 16, 'Z': 11, 'M': 7, 'T': 5, 'A': 3, 'H': 2, 'I': 2, 'N': 1, '0': 1, 'S': 1, 'J': 1, 'E': 1}), Counter({'1': 9, '0': 8, '6': 6, '7': 6, 'D': 4, 'I': 4, '4': 4, 'J': 4, 'Z': 3, '5': 3, 'E': 2, 'T': 2, '3': 2, '9': 2, 'N': 2, 'H': 1, 'P': 1, 'A': 1, 'X': 1, 'C': 1, '8': 1, 'B': 1}), Counter({'7': 9, '6': 6, '8': 5, '1': 4, '3': 4, '5': 3, '2': 3, '4': 3, '0': 2, '9': 1}), Counter({'8': 11, '3': 11, '9': 8, '2': 7, '4': 7, '1': 6, '6': 6, '7': 6, '0': 2}), Counter({'3': 14, '7': 8, '2': 6, '5': 6, '1': 5, '4': 5, '8': 4, '9': 4, '6': 4, '0': 4, 'B': 3, 'J': 2}), Counter({'4': 14, '5': 14, '8': 11, '3': 10, '7': 8, '9': 7, '6': 5, '1': 4, '0': 4, '2': 4, 'G': 2, 'C': 1})]\n",
        "    # print(c)\n",
        "    for i, x in enumerate(c):\n",
        "        freqword = defaultdict(list)\n",
        "        print(x)\n",
        "        for word, freq in x.items():\n",
        "            freqword[freq].append(word)\n",
        "\n",
        "        # print in order of occurrence (with sorted list of words)\n",
        "\n",
        "        wrong_char_in_column = 0\n",
        "        for freq in sorted(freqword,reverse=True):\n",
        "            print('col: {}- mistakes: {}: char: {}'.format(i+1, freq, sorted(freqword[freq])))\n",
        "            wrong_char_in_column += freq\n",
        "        print(\"column {} mistakes {}\".format(i+1,wrong_char_in_column))\n",
        "\n",
        "    freqword = {}\n",
        "    for i, x in enumerate(c):\n",
        "        for word, freq in x.items():\n",
        "            freqword[word] = 0\n",
        "\n",
        "    for i, x in enumerate(c):\n",
        "        for word, freq in x.items():\n",
        "            freqword[word] += freq\n",
        "\n",
        "        # print in order of occurrence (with sorted list of words)\n",
        "    sorted_x = sorted(freqword.items(), key=operator.itemgetter(1))\n",
        "    print(\"mistakes_together\", sorted_x)\n",
        "\n",
        "    y_map = [x for x in(string.digits+string.ascii_uppercase)]\n",
        "    print(y_map)\n",
        "\n",
        "    lst = []\n",
        "    for x in sorted_x:\n",
        "        for a in x:\n",
        "            lst.append(a)\n",
        "\n",
        "    print(lst)\n",
        "    zero_mistakes = list(set(y_map) - set(lst))\n",
        "    if all_labeled_chars:\n",
        "        characters_that_do_not_exist_in_dataset =  list(set(y_map) - set(all_labeled_chars))\n",
        "        print(\"1\",zero_mistakes)\n",
        "        print(\"2\",characters_that_do_not_exist_in_dataset)\n",
        "        print(\"zero mistakes: \", list(set(zero_mistakes) - (set(characters_that_do_not_exist_in_dataset))))\n",
        "    else:\n",
        "        print(\"zero mistakes: \", zero_mistakes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkDt0DsYCk-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# chars_stats_counter is only from the best index, if index of best epoch is wrong, you need to get chars_stats_counter manually\n",
        "if hdr:\n",
        "    print('=================hdr=============================')\n",
        "    all_labeled_chars = get_all_used_chararcters(hdr_dataset_labels)\n",
        "    character_analysis(chars_stats_counter_hdr[0], all_labeled_chars)\n",
        "# takes too long\n",
        "if reid:\n",
        "    print('=================reid============================')\n",
        "    all_labeled_chars = get_all_used_chararcters(reid_dataset_labels)\n",
        "    character_analysis(chars_stats_counter_reid[0], all_labeled_chars)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}